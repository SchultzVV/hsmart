# ğŸš€ Projeto de ProtÃ³tipo de LLM com Base de Conhecimento (Qdrant + FLAN-T5)

Este projeto implementa um **protÃ³tipo de microsserviÃ§os** que utiliza um **banco vetorial (Qdrant)** e um **modelo de LLM (`FLAN-T5`)** para responder perguntas com base em um conhecimento previamente armazenado.

---

## ğŸ“Œ **Arquitetura**
O projeto Ã© composto por trÃªs serviÃ§os principais:
1. **Vector Database (Qdrant)**: Armazena os embeddings dos textos processados.
2. **Ingestion Service** (`ingestion_service`): Extrai texto de uma pÃ¡gina, gera embeddings e armazena no Qdrant.
3. **Retrieval Service** (`retrieval_service`): Busca informaÃ§Ãµes relevantes no banco vetorial e usa um modelo de LLM (`FLAN-T5`) para gerar respostas.

---

## ğŸ“‚ **Estrutura de DiretÃ³rios**

```bash
/llm-projeto
â”‚â”€â”€ docker-compose.yaml
â”‚â”€â”€ .env
â”‚â”€â”€ README.md
â”‚â”€â”€ services/
â”‚   â”œâ”€â”€ ingestion_service/
â”‚   â”‚   â”œâ”€â”€ main.py  # API: ingestÃ£o de documentos
â”‚   â”‚   â”œâ”€â”€ requirements.txt  
â”‚   â”‚   â”œâ”€â”€ Dockerfile  
â”‚   â”œâ”€â”€ retrieval_service/
â”‚   â”‚   â”œâ”€â”€ main.py  # API: consulta e respostas
â”‚   â”‚   â”œâ”€â”€ requirements.txt  
â”‚   â”‚   â”œâ”€â”€ Dockerfile  
â”‚   â”‚   â”œâ”€â”€ .env  
â”‚â”€â”€ vector_db/
â”‚   â”œâ”€â”€ aliases/
â”‚   â”œâ”€â”€ collections/
â”‚   â”œâ”€â”€ docker-compose.yaml
â”‚   â”œâ”€â”€ config.py
â”‚â”€â”€ tests/
â”‚   â”œâ”€â”€ test_ingestion.py
â”‚   â”œâ”€â”€ test_retrieval.py
```


---

## ğŸš€ **Como Rodar o Projeto**

### **1ï¸âƒ£ Instale o Docker e Docker Compose**
- [Docker](https://docs.docker.com/get-docker/)
- [Docker Compose](https://docs.docker.com/compose/install/)


### **2ï¸âƒ£ Configure as VariÃ¡veis de Ambiente**
Para rodar o projeto, vocÃª precisarÃ¡ de um **Hugging Face Token**.  
Se ainda nÃ£o tem um, **crie um novo token de acesso aqui**:  
ğŸ”— [Criar Token no Hugging Face](https://huggingface.co/settings/tokens)

ApÃ³s gerar o token, crie o arquivo `.env` com o seguinte comando:
```bash
echo "HUGGINGFACE_TOKEN=seu_token_aqui" > .env
```
O arquivo `.env` deve estar na pasta raiz do projeto com o seguinte conteÃºdo:
> ğŸ”¹ **Substitua `seu_token_aqui` pelo token da Hugging Face** (necessÃ¡rio para baixar o modelo `FLAN-T5`).

### **3ï¸âƒ£ Suba os ContÃªineres**
```bash
docker-compose up --build
```
ApÃ³s concluir o passo 3 estÃ¡ pronto apra uso. A resposta para a pergunta do PDF foi gerada pelo endpoint e encontra-se no [`ultima_resposta.txt`](services/retrieval_service/ultima_resposta.txt). Para mais detalhes veja o [`README_de_resposta.md`](README_de_resposta.md)
## ğŸ§ª Como Rodar os Testes
Para garantir que os serviÃ§os estÃ£o funcionando corretamente, vocÃª pode rodar os testes automatizados.

#### Instale o `pytest` (se ainda nÃ£o estiver instalado)
```bash
pip install pytest
```
#### Realize os testes automÃ¡ticos
Rode o comando abaixo na pasta raiz do projeto
```bash
pytest tests/
```

O retorno esperado Ã© 
```bash
================= test session starts ================
collected 2 items

tests/test_ingestion.py âœ… PASSED
tests/test_retrieval.py âœ… PASSED
```

###### ** OBS: Se quiser ver os logs das imagens use os comandos**
```bash
docker logs ingestion_service --follow
```
ou
```bash
docker logs retrieval_service --follow
```
