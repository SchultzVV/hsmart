# ğŸ§  Retrieval Service â€” UFSM RAG com LangChain

ServiÃ§o responsÃ¡vel por responder perguntas com base em documentos previamente ingeridos, utilizando uma arquitetura de RecuperaÃ§Ã£o com GeraÃ§Ã£o (RAG) via LangChain, OpenAI e Qdrant.

---

## ğŸ“‘ Ãndice

- [ğŸ“Œ VisÃ£o Geral](#-visÃ£o-geral)
- [ğŸ§  Arquitetura LangChain](#-arquitetura-langchain)
- [ğŸ“‚ Arquivos Relevantes](#-arquivos-relevantes)
- [âš™ï¸ Como Funciona o Ciclo RAG](#ï¸-como-funciona-o-ciclo-rag)
- [ğŸ“¥ IngestÃ£o de ConteÃºdo](#-ingestÃ£o-de-conteÃºdo)
- [ğŸ§ª Como Testar](#-como-testar)
- [ğŸ” Debug do Processo](#-debug-do-processo)
- [ğŸ“ Links Ãšteis](#-links-Ãºteis)

---

## ğŸ“Œ VisÃ£o Geral

Este serviÃ§o implementa um sistema RAG (Retrieval-Augmented Generation), capaz de:

- Identificar a melhor coleÃ§Ã£o vetorial para a pergunta.
- Recuperar documentos similares via Qdrant.
- Utilizar `ChatOpenAI` para responder com base nesses contextos.

---

## ğŸ§  Arquitetura LangChain

| Componente        | Classe                                   | DescriÃ§Ã£o                                                         |
|------------------|------------------------------------------|-------------------------------------------------------------------|
| Embeddings        | `OpenAIEmbeddings`                       | Gera vetores numÃ©ricos para textos                                |
| LLM               | `ChatOpenAI`                             | Modelo de linguagem usado para resposta contextual                |
| Vetor Store       | `Qdrant`                                 | Banco vetorial onde os documentos sÃ£o armazenados                 |
| Retriever         | `vectorstore.as_retriever()`             | Busca os documentos mais relevantes                               |
| Cadeia QA         | `RetrievalQA.from_chain_type(...)`       | Executa o fluxo de recuperaÃ§Ã£o + geraÃ§Ã£o                          |
| Splitter          | `RecursiveCharacterTextSplitter`         | Divide o texto em chunks para vetorizaÃ§Ã£o                         |

ğŸ“š DocumentaÃ§Ã£o:
- [OpenAIEmbeddings](https://python.langchain.com/docs/integrations/text_embedding/openai)
- [ChatOpenAI](https://python.langchain.com/docs/integrations/llms/openai)
- [Qdrant VectorStore](https://python.langchain.com/docs/integrations/vectorstores/qdrant)
- [RetrievalQA](https://python.langchain.com/docs/modules/chains/popular/retrieval_qa)
- [Text Splitters](https://python.langchain.com/docs/modules/data_connection/document_transformers/text_splitters/)

---

## ğŸ“‚ Arquivos Relevantes

| Arquivo                         | FunÃ§Ã£o                                                            |
|----------------------------------|-------------------------------------------------------------------|
| `langchain_container.py`         | Inicializa embeddings, LLM, vectorstore, retriever, QAChain      |
| `collection_router.py`           | Decide qual coleÃ§Ã£o usar com heurÃ­stica + vetorial               |
| `qa_service.py`                  | Executa a cadeia RAG e loga as fontes usadas                      |
| `query_routes.py`                | ExpÃµe o endpoint `/query`                                        |

---

## âš™ï¸ Como Funciona o Ciclo RAG

```mermaid
graph TD
    A[UsuÃ¡rio envia pergunta] --> B[qa_service.py]
    B --> C[CollectionRouter decide coleÃ§Ã£o]
    C --> D[set_collection()]
    D --> E[Retriever busca chunks no Qdrant]
    E --> F[ChatOpenAI responde com contexto]
    F --> G[Retorna resposta + fontes]
```

---

## ğŸ“¥ IngestÃ£o de ConteÃºdo

- Split por `RecursiveCharacterTextSplitter` com `chunk_size=512`, `overlap=64`.
- Armazena no Qdrant via `vectorstore.add_documents()`.
- Cada chunk contÃ©m metadados Ãºteis:
  - `curso`, `campus`, `document_title`, `source`, `timestamp`.

---

## ğŸ§ª Como Testar

### Fazer uma pergunta

```bash
curl -X POST http://localhost:5004/query \
  -H "Content-Type: application/json" \
  -d '{"question": "qual Ã© o roteiro para elaboracao de projeto de ensino de arquitetura ?"}'
```

---

## ğŸ” Debug do Processo

- `qa_service.py` faz `print()` dos documentos fontes:
  ```python
  for doc in result["source_documents"]:
      print(doc.page_content[:300])
      print(doc.metadata)
  ```
- VocÃª pode inspecionar coleÃ§Ãµes com:
  ```python
  container.qdrant_client.scroll(collection_name="ufsm_knowledge", limit=5)
  ```

---

## ğŸ“ Links Ãšteis

- LangChain docs: https://docs.langchain.com
- LangChain GitHub: https://github.com/langchain-ai/langchain
- Qdrant docs: https://qdrant.tech/documentation
- OpenAI API: https://platform.openai.com/docs
