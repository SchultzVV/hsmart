# ğŸ“˜ DefineMe.md â€” Concepts Behind UFSM RAG

This document defines and explains key mathematical and conceptual foundations of Retrieval-Augmented Generation (RAG), focusing on **embeddings**, **vector similarity**, and **vector database storage**.

---

## ğŸ“‘ Table of Contents

- [ğŸ§  Embeddings](#-embeddings)
  - [ğŸ”¤ What is an Embedding?](#-what-is-an-embedding)
  - [ğŸ“ Embedding as a Vector](#-embedding-as-a-vector)
  - [ğŸ“ Measuring Similarity: Cosine Distance](#-measuring-similarity-cosine-distance)
  - [ğŸ“Š Embeddings in Vector Space (Diagram)](#-embeddings-in-vector-space-diagram)
  - [âœï¸ Why Chunking Improves Embedding Quality](#ï¸-why-chunking-improves-embedding-quality)
  - [ğŸ§® Vector Distance: Similar vs Dissimilar Texts](#-vector-distance-similar-vs-dissimilar-texts)
  - [ğŸ§¬ Eigenvectors and Vector Projections](#-eigenvectors-and-vector-projections)
  - [ğŸ“‰ SVD vs PCA for Embedding Reduction](#-svd-vs-pca-for-embedding-reduction)
  - [ğŸ“ˆ t-SNE / UMAP Visualizations](#-t-sne--umap-visualizations)
  - [ğŸ“ Angular vs Euclidean Distance in High Dimensions](#-angular-vs-euclidean-distance-in-high-dimensions)
- [ğŸ’¾ Vector Databases](#-vector-databases)
  - [ğŸ’¾ How a Vector is Stored in Qdrant](#-how-a-vector-is-stored-in-qdrant)
  - [ğŸ” Vector Indexes in Practice](#-vector-indexes-in-practice)
  - [ğŸ•¸ï¸ HNSW Internals (Hierarchical Navigable Small World)](#ï¸-hnsw-internals-hierarchical-navigable-small-world)
- [ğŸ¯ Semantic Search](#-semantic-search)
  - [ğŸ¯ Metric Learning (Why Some Distances Work Better)](#-metric-learning-why-some-distances-work-better)
  - [ğŸ” When a User Asks a Question](#-when-a-user-asks-a-question)
- [ğŸ”® Coming Next](#-coming-next)

---

## ğŸ§  Embeddings

### ğŸ”¤ What is an Embedding?

An **embedding** is a numerical representation of a text (sentence, document, word) in a continuous vector space.

- Embeddings allow **semantic similarity** to be computed as **vector distance**.
- Generated using language models like OpenAI (`text-embedding-3-small`), HuggingFace Transformers, etc.

---

### ğŸ“ Embedding as a Vector

An embedding is a list of floating-point numbers. For example:

```python
[0.041, -0.238, 0.501, ..., 0.019]
```

- Typical size: 384, 768, 1536, or 3072 dimensions depending on model.

---

### ğŸ“ Measuring Similarity: Cosine Distance

Let vectors **A** and **B** represent two texts. Their similarity is:

```math
cos(Î¸) = (A Â· B) / (||A|| * ||B||)
```

- `A Â· B` is the dot product
- `||A||` and `||B||` are vector magnitudes

```math
cosine_distance = 1 - cosine_similarity
```

---

### ğŸ“Š Embeddings in Vector Space (Diagram)

Imagine texts placed in a 3D space (simplified):

```
           ğŸ“˜ Text A ("How to enroll")
              *
             / \
            /   \
           *     * ğŸ“™ Text B ("Steps to register")
          ğŸ“• Text C ("Pizza toppings")
```

- Text A and B are **close**, high semantic similarity
- Text C is **far**, different topic
- In actual models this occurs in 1536D space, not 3D

---

### âœï¸ Why Chunking Improves Embedding Quality

**Chunking** breaks long texts into smaller segments, helping in:

- Preserving context size within LLM token limits
- Enhancing vector precision (more specific = better embeddings)
- Avoiding dilution: "big blobs" often confuse the embedding model

Example:

```
Long text (bad):
"UFSM offers many courses including architecture, law, biology..."

Chunked (good):
"Architecture is a course at UFSM focused on..."

"Law at UFSM prepares students for..."

"Biology involves research on ecosystems..."
```

---

### ğŸ§® Vector Distance: Similar vs Dissimilar Texts

Letâ€™s compare cosine distances:

| Text A vs B                                    | Cosine Distance |
|------------------------------------------------|------------------|
| "How to apply for architecture?" vs "How to enroll in UFSM?" | **0.12** *(very close)* |
| "Architecture course details" vs "Pizza recipe instructions" | **0.87** *(very far)* |

Lower = closer in meaning.  
Higher = semantically different.

---

### ğŸ§¬ Eigenvectors and Vector Projections

While embeddings are high-dimensional vectors, they can be understood through linear algebra.

- **Eigenvectors** describe the *directions* along which transformations (like PCA or matrix ops) scale without rotating.
- In embeddings, **principal components** can be derived using eigenvectors of the covariance matrix â€” useful for visualization and compression.

#### ğŸ”¢ Projection onto Direction

Project vector **v** onto direction **u** (unit vector):

```math
proj_u(v) = (v Â· u) * u
```

---

### ğŸ“‰ SVD vs PCA for Embedding Reduction

## ğŸ”¹ SVD â€” Singular Value Decomposition

Uma tÃ©cnica matemÃ¡tica geral que decompÃµe qualquer matriz em trÃªs partes:

```math
X = U Â· Î£ Â· Váµ€
```

- **X**: matriz original (ex: embeddings de documentos)
- **U**: vetores ortogonais que representam direÃ§Ãµes dos dados
- **Î£**: valores singulares â€” quanto cada direÃ§Ã£o contribui
- **Váµ€**: rotaÃ§Ã£o e alinhamento final

âœ… Usado em Ã¡lgebra linear para compressÃ£o, reduÃ§Ã£o de ruÃ­do, recomendaÃ§Ã£o, etc.  
âœ… Pode ser aplicado mesmo em matrizes nÃ£o centradas ou retangulares.

---

## ğŸ”¹ PCA â€” Principal Component Analysis

Uma tÃ©cnica estatÃ­stica que identifica as direÃ§Ãµes com maior variÃ¢ncia nos dados (componentes principais).

- PCA encontra **vetores prÃ³prios (eigenvectors)** da matriz de covariÃ¢ncia dos dados.
- Esses vetores **maximizam a variaÃ§Ã£o** dos dados projetados.
- Funciona basicamente como um **SVD da matriz centrada** (X - mÃ©dia).

âœ… PCA Ã© um **caso especÃ­fico de SVD**, onde os dados sÃ£o centrados e reduzidos a componentes mais informativos.

---

## ğŸ§  Na prÃ¡tica para embeddings:

|                     | PCA         | SVD         |
|---------------------|-------------|-------------|
| Para dados centrados| âœ… Ideal    | âœ… Funciona |
| Dados esparsos      | âŒ NÃ£o ideal| âœ… Sim      |
| ReduÃ§Ã£o de dimensÃµes| âœ… Sim      | âœ… Sim      |
| InterpretaÃ§Ã£o estatÃ­stica | âœ… Boa | âŒ MatemÃ¡tica apenas |
---
**Overview**
| Method | Description |
|--------|-------------|
| **PCA** | Finds directions (principal components) that maximize variance. |
| **SVD** | General matrix decomposition: X = UÎ£Váµ—. More flexible than PCA. |

Used to reduce high-dimensional embeddings into fewer, denser dimensions for performance or visualization.

---

### ğŸ“ˆ t-SNE / UMAP Visualizations

**t-SNE** and **UMAP** are nonlinear dimensionality reduction techniques for visualizing embeddings.

| Method | Notes |
|--------|-------|
| t-SNE  | Great for local structure, less on global | 
| UMAP   | Better balance between local/global, faster |

---

### ğŸ“ Angular vs Euclidean Distance in High Dimensions

| Feature         | Cosine         | Euclidean     |
|----------------|----------------|---------------|
| Magnitude-sensitive | âŒ No           | âœ… Yes         |
| Direction-sensitive | âœ… Yes         | âœ… Yes         |
| Works well in NLP   | âœ… Yes         | âŒ No          |
| RAG-recommended     | âœ… Yes         | âŒ Not ideal   |

---

## ğŸ’¾ Vector Databases

### ğŸ’¾ How a Vector is Stored in Qdrant

```json
{
  "id": 3281,
  "vector": [0.041, -0.238, 0.501, ...],
  "payload": {
    "curso": "Arquitetura e Urbanismo",
    "source": "https://www.ufsm.br/...",
    "text": "...",
    "timestamp": "2025-06-20T14:30:00"
  }
}
```

---

### ğŸ” Vector Indexes in Practice

| Index Type | Description                                      |
|------------|--------------------------------------------------|
| HNSW       | Hierarchical Navigable Small World (graph-based) |
| IVF        | Inverted File Index (clustered)                  |
| Flat       | Brute-force (slow but exact)                     |

---

### ğŸ•¸ï¸ HNSW Internals (Hierarchical Navigable Small World)

- Multi-layer graph for fast approximate search
- Greedy traversal from top-level entry point down
- Tunable via: `ef_construction`, `ef_search`, `m`

---

## ğŸ¯ Semantic Search

### ğŸ¯ Metric Learning (Why Some Distances Work Better)

Trains embedding models so that **semantic similarity = geometric closeness**.

| Type            | Description |
|-----------------|-------------|
| Supervised      | Learns from labeled pairs |
| Self-supervised | Contrastive learning (e.g. BERT-NLI, SimCLR) |

---

### ğŸ” When a User Asks a Question

1. Question is embedded into a vector.
2. Qdrant retrieves nearest chunks.
3. Context is passed to LLM (e.g., GPT).
4. Response is generated grounded in context.

---

## ğŸ”® Coming Next

Would you like the next section to cover:

- ğŸ§± Embedding compression strategies?
- ğŸ§² Retrieval performance tuning?
- ğŸ¯ Evaluation metrics for RAG (Precision@K, Exact Match, Recall@K)?

Let me know and Iâ€™ll add the next block!
